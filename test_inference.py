from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# âœ… Path to your fine-tuned LoRA model
model_id = "data/mba/fine_tune/artifacts/lora/qwen2_5_1.5b_REAL"

print("ğŸ”„ Loading model...")
pipe = pipeline("text-generation", model=model_id, tokenizer=model_id)
print("âœ… Model loaded successfully!")

# ğŸ§¾ Example test resume
prompt = """You are a professional resume normalizer. 
Output ONLY a valid JSON object following this schema:

{
  "tier": "tier2_mid",
  "career": {
     "total_years": 5,
     "current_role": "Analyst",
     "role_level": "associate"
  },
  "industry": {
     "sector": "Consulting",
     "company_tier": 2
  },
  "signals": {
     "leadership": true,
     "impact": true,
     "international": false,
     "tools": ["Excel", "SQL"]
  }
}

Now label this resume into categories:
Resume:
Analyst with ~5 years' experience in Consulting; educated at NMIMS Mumbai; focus: Strategy.
JSON:
"""

# âš™ï¸ Generate structured output
print("\nâš™ï¸ Generating structured JSON output...\n")
result = pipe(prompt, max_new_tokens=400, do_sample=False)[0]["generated_text"]

print("=" * 80)
print("RAW MODEL OUTPUT:\n")
print(result)
print("=" * 80)
