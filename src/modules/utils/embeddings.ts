/**
 * ðŸš€ embeddings.ts
 * --------------------------------------------------------
 * Production-grade embedding + similarity utility.
 *
 * Supports:
 *  âœ… OpenAI embeddings (fast + reliable)
 *  âœ… Local fallback (Sentence Transformers)
 *  âœ… Caching (in-memory & file-based for re-use)
 *  âœ… Cosine similarity computation
 *
 * Designed for:
 *  - Preprocessing semantic normalization
 *  - LoRA / QLoRA fine-tuning data prep
 *  - Resume understanding, alias mapping, clustering
 */

import fs from "fs";
import path from "path";
import crypto from "crypto";

// --- Config ---
const USE_OPENAI = process.env.USE_OPENAI_EMBEDDINGS === "true";
const OPENAI_MODEL = process.env.OPENAI_EMBEDDING_MODEL || "text-embedding-3-small";
const LOCAL_MODEL = "all-MiniLM-L6-v2"; // best speed:accuracy tradeoff

// Cache dir for reusing embeddings between runs
const CACHE_DIR = path.resolve(process.cwd(), ".cache/embeddings");
if (!fs.existsSync(CACHE_DIR)) fs.mkdirSync(CACHE_DIR, { recursive: true });

// --- Dynamic imports (lazy to keep main bundle light) ---
let openaiClient: any;
let sentenceModel: any;

/**
 * Generate hash for text (used for caching)
 */
function hashText(text: string): string {
  return crypto.createHash("md5").update(text.trim().toLowerCase()).digest("hex");
}

/**
 * Load cached embedding if available
 */
function loadCachedEmbedding(hash: string): number[] | null {
  const filePath = path.join(CACHE_DIR, `${hash}.json`);
  if (fs.existsSync(filePath)) {
    try {
      const parsed = JSON.parse(fs.readFileSync(filePath, "utf-8"));
      return parsed.embedding;
    } catch {
      return null;
    }
  }
  return null;
}

/**
 * Save embedding to cache
 */
function saveCachedEmbedding(hash: string, embedding: number[]) {
  const filePath = path.join(CACHE_DIR, `${hash}.json`);
  fs.writeFileSync(filePath, JSON.stringify({ embedding }), "utf-8");
}

/**
 * ðŸ§  Main: Get Embedding (auto chooses best backend)
 */
export async function embedText(text: string): Promise<number[]> {
  const trimmed = text.trim();
  if (!trimmed) return [];

  const hash = hashText(trimmed);
  const cached = loadCachedEmbedding(hash);
  if (cached) return cached;

  let embedding: number[] = [];

  try {
    // --- Option 1: OpenAI embeddings ---
    if (USE_OPENAI) {
      if (!openaiClient) {
        const OpenAI = (await import("openai")).default;
        openaiClient = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      }

      const res = await openaiClient.embeddings.create({
        model: OPENAI_MODEL,
        input: trimmed
      });
      embedding = res.data[0].embedding;
    }

    // --- Option 2: Local transformer fallback ---
    else {
      if (!sentenceModel) {
        const { pipeline } = await import("@xenova/transformers");
        sentenceModel = await pipeline("feature-extraction", LOCAL_MODEL);
      }
      const output = await sentenceModel(trimmed);
      embedding = Array.from(output.data[0]);
    }

    // Save for re-use
    saveCachedEmbedding(hash, embedding);
    return embedding;
  } catch (err) {
    console.error("[Embeddings] Failed to embed:", err);
    return [];
  }
}

/**
 * ðŸ“ˆ Compute cosine similarity between two embeddings
 */
export function cosineSimilarity(a: number[], b: number[]): number {
  if (a.length === 0 || b.length === 0) return 0;
  const dot = a.reduce((sum, ai, i) => sum + ai * (b[i] || 0), 0);
  const normA = Math.sqrt(a.reduce((sum, ai) => sum + ai * ai, 0));
  const normB = Math.sqrt(b.reduce((sum, bi) => sum + bi * bi, 0));
  return normA && normB ? dot / (normA * normB) : 0;
}

/**
 * ðŸ§ª CLI Test
 * Example: npx tsx modules/utils/embeddings.ts "lead a team" "manage a group"
 */
if (require.main === module) {
  (async () => {
    const [textA, textB] = process.argv.slice(2);
    if (!textA || !textB) {
      console.log("Usage: npx tsx modules/utils/embeddings.ts 'textA' 'textB'");
      process.exit(0);
    }

    const embA = await embedText(textA);
    const embB = await embedText(textB);
    const sim = cosineSimilarity(embA, embB);

    console.log(`\nðŸ§  A: ${textA}`);
    console.log(`ðŸ§  B: ${textB}`);
    console.log(`âœ… Cosine Similarity: ${sim.toFixed(3)}\n`);
  })();
}
